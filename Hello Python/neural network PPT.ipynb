{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_x = np.array([\n",
    "    [0,0,1],\n",
    "    [0,1,1],\n",
    "    [1,0,1],\n",
    "    [1,1,1],\n",
    "    [0,0,0],\n",
    "    [0,1,0],\n",
    "    [1,0,0],\n",
    "    [1,1,0]\n",
    "])\n",
    "train_y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sigmoid(x,der = False):\n",
    "    \n",
    "    if der == True:\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def _tanh(x,der = False):\n",
    "    \n",
    "    if der == True:\n",
    "        return 1-(x*x)\n",
    "\n",
    "    return np.tanh(x)\n",
    "\n",
    "def _relu(x,der = False):\n",
    "    \n",
    "    if der ==True:\n",
    "        return 1*(x>0)\n",
    "    \n",
    "    return x*(x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activation = _sigmoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(parameter,x):\n",
    "    \n",
    "    w0,w1 = parameter\n",
    "\n",
    "    l0 = np.array(x+[1])                     # (batch_size,i)\n",
    "    l1 = activation(l0.dot(w0))              # (batch_size,h)\n",
    "    l2 = activation(l1.dot(w1))              # (batch_size,o)\n",
    "\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Back propagation\n",
    "##    using random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w0 = 2*np.random.random((4,4))-1\n",
    "w1 = 2*np.random.random((4,1))-1\n",
    "parameter = (w0,w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference(parameter,[0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation: chain rule & gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient descent\n",
    "\n",
    " if f(x) is continuously differentiable function\n",
    " \n",
    "\n",
    " $f(x^{t+2})<f(x^{t+1})<f(x^t), t = 0,1,2...$ \n",
    " \n",
    " then f(x) turn to local mimimum\n",
    " \n",
    " How to find $x^{t+1}$ ?\n",
    "\n",
    "### $$f(x+h) = f(x)+f'(x)*(x+h-x)+\\frac{f''(x)*h*h}{2!}+\\frac{f'''(x)*h*h*h}{3!}+...........$$\n",
    "\n",
    "### hence $$f(x+\\Delta x)  â‰ˆ f(x)+\\Delta x*f'(x)$$\n",
    "\n",
    "we want $f(x+\\Delta x)<f(x)$ \n",
    "\n",
    "hence \n",
    "\n",
    "$\\Delta x = - \\eta * f'(x)$\n",
    "\n",
    "namely\n",
    "\n",
    "$ w_{hj} = w_{hj} - \\eta  \\frac{\\partial loss}{\\partial w_{hj}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain rule\n",
    "\n",
    "$$\\frac{\\partial loss}{\\partial w_{hj}} = \\frac{\\partial loss}{\\partial y} \\frac{\\partial y}{\\partial l_h} \\frac{\\partial l_h}{\\partial w_{hj}}$$\n",
    "\n",
    "$$ \\frac{\\partial h}{\\partial whj} = l1,l2..$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L2\n",
    "\n",
    "def l2_train():\n",
    "    \n",
    "    i = 3+1\n",
    "    o = 1\n",
    "    \n",
    "    # hyperparameter\n",
    "\n",
    "    num = 50000\n",
    "    h = 4\n",
    "    batch_size = 8 \n",
    "    #stochasitc gradient descent & minibatch gradient descent\n",
    "    \n",
    "    \n",
    "    # add_b\n",
    "    x_with_b = np.column_stack((train_x,np.ones(batch_size)))\n",
    "    \n",
    "    #parameter\n",
    "    w0 = 2*np.random.random((i,h))-1\n",
    "    w1 = 2*np.random.random((h,o))-1\n",
    "    \n",
    "\n",
    "    for epoch in range(num):\n",
    "\n",
    "        # feed forward\n",
    "        l0 = x_with_b                              # (batch_size,i)\n",
    "        l1 = activation(l0.dot(w0))                # (batch_size,h)\n",
    "        l2 = activation(l1.dot(w1))                # (batch_size,o)\n",
    "\n",
    "        #loss\n",
    "        loss = 1/2*(train_y - l2)**2\n",
    "        \n",
    "        # back propagate\n",
    "        l2_err = train_y - l2                      # (batch_size,o)\n",
    "        l2_delta = l2_err*activation(l2,der=True)  # (batch_size,o)\n",
    "        \n",
    "        l1_err = l2_delta.dot(w1.T)                # (batch_size,h)\n",
    "        l1_delta = l1_err*activation(l1,der=True)  # (batch_size,h)\n",
    "\n",
    "        ## update parameter\n",
    "        w1 += l1.T.dot(l2_delta)\n",
    "        w0 += l0.T.dot(l1_delta)\n",
    "        \n",
    "        if epoch%1000 == 0:\n",
    "            print(np.mean(loss))\n",
    "\n",
    "        parameter = (w0,w1)\n",
    "        \n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross-entropy\n",
    "\n",
    "def cross_entropy_train():\n",
    "    \n",
    "    i = 3+1\n",
    "    o = 1\n",
    "    \n",
    "    # hyperparameter\n",
    "\n",
    "    num = 50000\n",
    "    h = 4\n",
    "    batch_size = 8\n",
    "    \n",
    "    # add_b\n",
    "    x_with_b = np.column_stack((train_x,np.ones(batch_size)))\n",
    "    \n",
    "    #parameter\n",
    "    w0 = 2*np.random.random((i,h))-1\n",
    "    w1 = 2*np.random.random((h,o))-1\n",
    "    \n",
    "\n",
    "    for epoch in range(num):\n",
    "\n",
    "        # feed forward\n",
    "        l0 = x_with_b                           # (batch_size,i)\n",
    "        l1 = activation(l0.dot(w0))             # (batch_size,h)\n",
    "        l2 = activation(l1.dot(w1))             # (batch_size,o)\n",
    "\n",
    "        #loss\n",
    "        loss = train_y*np.log(l2) + (1-train_y)*np.log(1-l2)\n",
    "        \n",
    "        # back propagate\n",
    "        # l2_err = (train_y - l2)/(l2*(1-l2))        # (batch_size,o)\n",
    "        # l2_delta = l2_err*activation(l2,der=True)  # (batch_size,o)\n",
    "        \n",
    "        l2_delta = train_y - l2\n",
    "        \n",
    "        l1_err = l2_delta.dot(w1.T)                # (batch_size,h)\n",
    "        l1_delta = l1_err*activation(l1,der=True)  # (batch_size,h)\n",
    "        \n",
    "        \n",
    "\n",
    "        ## update parameter\n",
    "        w1 += l1.T.dot(l2_delta)\n",
    "        w0 += l0.T.dot(l1_delta)\n",
    "        \n",
    "        if epoch%1000 == 0:\n",
    "            print(np.mean(loss))\n",
    "\n",
    "        parameter = (w0,w1)\n",
    "        \n",
    "    return parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference(l2_train(),[0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inference(cross_entropy_train(),[0,1,1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
