{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_x = np.array([\n",
    "    [0,0,1],\n",
    "    [0,1,1],\n",
    "    [1,0,1],\n",
    "    [1,1,1],\n",
    "    [0,0,0],\n",
    "    [0,1,0],\n",
    "    [1,0,0],\n",
    "    [1,1,0]\n",
    "])\n",
    "train_y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "def _sigmoid(x,der = False):\n",
    "    \n",
    "    if der == True:\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def _tanh(x,der = False):\n",
    "    \n",
    "    if der == True:\n",
    "        return 1-(x*x)\n",
    "\n",
    "    return np.tanh(x)\n",
    "\n",
    "def _relu(x,der = False):\n",
    "    \n",
    "    if der ==True:\n",
    "        return 1*(x>0)\n",
    "    \n",
    "    return x*(x>0)\n",
    "\n",
    "\n",
    "activation = _sigmoid \n",
    "\n",
    "def train():\n",
    "\n",
    "    \n",
    "    \n",
    "    i = 3+1\n",
    "    o = 1\n",
    "    \n",
    "    # hyperparameter\n",
    "\n",
    "    num = 50000\n",
    "    h = 4\n",
    "    batch_size = 8\n",
    "    \n",
    "    # add_b\n",
    "    x_with_b = np.column_stack((train_x,np.ones(batch_size)))\n",
    "    \n",
    "    #parameter\n",
    "    w0 = 2*np.random.random((i,h))-1\n",
    "    w1 = 2*np.random.random((h,o))-1\n",
    "    \n",
    "\n",
    "    for epoch in range(num):\n",
    "\n",
    "        # feed forward\n",
    "        l0 = x_with_b                            # (batch_size,i)\n",
    "        l1 = activation(l0.dot(w0))             # (batch_size,h)\n",
    "        l2 = activation(l1.dot(w1))             # (batch_size,o)\n",
    "\n",
    "        #loss\n",
    "        loss = 1/2*(train_y - l2)**2\n",
    "        \n",
    "        # back propagate\n",
    "        l2_err = train_y - l2                      # (batch_size,o)\n",
    "        l2_delta = l2_err*activation(l2,der=True)  # (batch_size,o)\n",
    "        \n",
    "        l1_err = l2_delta.dot(w1.T)                # (batch_size,h)\n",
    "        l1_delta = l1_err*activation(l1,der=True)  # (batch_size,h)\n",
    "\n",
    "        ## update parameter\n",
    "        w1 += l1.T.dot(l2_delta)\n",
    "        w0 += l0.T.dot(l1_delta)\n",
    "        \n",
    "        if epoch%1000 == 0:\n",
    "            print(np.mean(loss))\n",
    "\n",
    "        parameter = (w0,w1)\n",
    "        \n",
    "    return parameter\n",
    "\n",
    "def inference(parameter,x):\n",
    "    \n",
    "        w0,w1 = parameter\n",
    "\n",
    "        l0 = np.array(x+[1])                     # (batch_size,i)\n",
    "        l1 = activation(l0.dot(w0))              # (batch_size,h)\n",
    "        l2 = activation(l1.dot(w1))              # (batch_size,o)\n",
    "\n",
    "        return l2\n",
    "\n",
    "test_x = [0,1,1]\n",
    "\n",
    "parameter = train()\n",
    "pre_y = inference(parameter,test_x)\n",
    "print(pre_y)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
